{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "like_contrastive_first_cont_loss_then_nn.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzZbjajwB8e7ahEi6dzRSf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tayfununal/continuous_CNN/blob/main/like_contrastive_first_cont_loss_then_nn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVA1Pg1plFdn"
      },
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import Model, Input\n",
        "from tensorflow.keras.layers import Flatten, Dense\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B6qnPo8YlPnt"
      },
      "source": [
        "# Downloading dataset\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "\n",
        "print(\"X_train : \",X_train.shape, \" - type : \",type(X_train))\n",
        "print(\"y_train : \",y_train.shape)\n",
        "print(\"X_test : \",X_test.shape)\n",
        "print(\"y_test : \",y_test.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wb0mkS9glRuk"
      },
      "source": [
        "# Herbir sınıftan 1000 veri alarak toplam veri kümesini 10 000 yaptık.\n",
        "\n",
        "x = X_train[np.where(y_train==0)[0][:1000]]\n",
        "y = y_train[np.where(y_train==0)[0][:1000]]\n",
        "\n",
        "x_t =  X_test[np.where(y_test==0)[0][:200]]\n",
        "y_t = y_test[np.where(y_test==0)[0][:200]]\n",
        "\n",
        "for i in range(1,10):\n",
        "  x_ek = X_train[np.where(y_train==i)[0][:1000]]\n",
        "  x = np.concatenate((x, x_ek))\n",
        "\n",
        "  y_ek = y_train[np.where(y_train==i)[0][:1000]]\n",
        "  y = np.concatenate((y, y_ek))\n",
        "\n",
        "  x_t_ek = X_test[np.where(y_test==i)[0][:200]]\n",
        "  x_t = np.concatenate((x_t, x_t_ek))\n",
        "\n",
        "  y_t_ek = y_test[np.where(y_test==i)[0][:200]]\n",
        "  y_t = np.concatenate((y_t, y_t_ek))\n",
        "\n",
        "print(\"Shape of x :\", x.shape)\n",
        "print(\"Shape of y :\", y.shape)\n",
        "print(\"Shape of x_t :\", x_t.shape)\n",
        "print(\"Shape of y_t :\", y_t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P-X4gD4lUTs"
      },
      "source": [
        "x = x.reshape(x.shape[0], 28, 28, 1)\n",
        "x_t = x_t.reshape(x_t.shape[0], 28, 28, 1)\n",
        "\n",
        "x = x.astype('float32')\n",
        "x_t = x_t.astype('float32')\n",
        "\n",
        "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
        "x /= 255\n",
        "x_t /= 255\n",
        "\n",
        "print(\"Shape of x :\", x.shape)\n",
        "print(\"Shape of x_t :\", x_t.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIk0n-mZlX9s"
      },
      "source": [
        "# Adding epsilon\n",
        "tf.random.set_seed(8)\n",
        "epsilon = tf.random.normal((10000,28,28,1), mean=0, stddev= 0.00001)\n",
        "epsilon_test = tf.random.normal((2000,28,28,1), mean=0, stddev= 0.00001)\n",
        "\n",
        "X_train_with_epsilon = tf.math.add(x, epsilon)\n",
        "X_test_with_epsilon = tf.math.add(x_t, epsilon_test)\n",
        "\n",
        "print('x shape:', X_train_with_epsilon.shape)\n",
        "print('Number of images in x', X_train_with_epsilon.shape[0])\n",
        "print('Number of images in x_t', X_test_with_epsilon.shape[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SrOt6Vs0lZ98"
      },
      "source": [
        "# Making y_train and y_test to categorical\n",
        "y = to_categorical(y)\n",
        "y_t = to_categorical(y_t)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_wK5iXjliBt"
      },
      "source": [
        "np.random.seed(8)\n",
        "tf.random.set_seed(8)\n",
        "\n",
        "feature_input = Input(shape=(28,28))\n",
        "\n",
        "hidden = Flatten()(feature_input)\n",
        "hidden = Dense(512, activation=\"relu\", name=\"one\")(hidden)\n",
        "hidden = Dense(512, activation=\"relu\")(hidden)\n",
        "\n",
        "output = Dense(10)(hidden)\n",
        "\n",
        "model_1 = Model(inputs=feature_input , outputs=output)\n",
        "model_1.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEdXv5C9l9Kl"
      },
      "source": [
        "batch_size = 10000\n",
        "lamdas =100\n",
        "\n",
        "class CustomMSE(tf.keras.losses.Loss):\n",
        "\n",
        "    def __init__(self, regularization_factor=0.1, name=\"custom_mse\"):\n",
        "        super().__init__(name=name)\n",
        "        self.regularization_factor = regularization_factor\n",
        "\n",
        "    def call(self, y_true, y_pred):\n",
        "\n",
        "        loss = lamdas * (1/batch_size) * tf.reduce_sum(abs(y_pred - model_1(X_train_with_epsilon)))\n",
        "\n",
        "        return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DHOaJdD9mHmk"
      },
      "source": [
        "# Culculating Custom Loss\n",
        "model_1.compile(optimizer=\"Adam\", loss=CustomMSE(), metrics=[\"accuracy\"])\n",
        "history = model_1.fit(x, y, epochs=50, verbose=1, batch_size=10000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6pxc3uguiCj"
      },
      "source": [
        "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "\n",
        "# Making sure that the values are float so that we can get decimal points after division\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
        "X_train /= 255\n",
        "X_test /= 255\n",
        "\n",
        "y_train = to_categorical(y_train)\n",
        "y_test = to_categorical(y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAryvgXYyzGh"
      },
      "source": [
        "model_1.get_weights()[4].shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oa68c_qVxwwb"
      },
      "source": [
        "np.random.seed(8)\n",
        "tf.random.set_seed(8)\n",
        "\n",
        "feature_input = Input(shape=(28,28))\n",
        "\n",
        "hidden = Flatten()(feature_input)\n",
        "hidden = Dense(512, activation=\"relu\")(hidden)\n",
        "hidden = Dense(512, activation=\"relu\")(hidden)\n",
        "\n",
        "output = Dense(10, activation=\"softmax\")(hidden)\n",
        "\n",
        "model_2 = Model(inputs=feature_input , outputs=output)\n",
        "model_2.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqwnvmTU4TkS"
      },
      "source": [
        "model_2.set_weights(model_1.get_weights())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJcSWNtumfnZ"
      },
      "source": [
        "# Culculating Custom Loss\n",
        "model_2.compile(optimizer=\"Adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "history = model_2.fit(X_train, y_train, epochs=50, verbose=1, batch_size=60000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_m5CJORnv4AZ"
      },
      "source": [
        "# model.evaluate(x_t,  y_t, batch_size=2000, verbose=1)  -- reg'de model(X_train_with_epsilon) 10 000 boyutlu olduğundan sıkıntı çıkarıyor çünkü burada y_pred 2000\n",
        "model_2.evaluate(X_test,  y_test, batch_size=10000, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kT3ypkS60wH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}